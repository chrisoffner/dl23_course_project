{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualise cross-attention and self-attention maps\n",
        "\n",
        "In this notebook we visualise both self-attention and cross-attention maps and observe how they change as we modify the input `context`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following cell ensures that the Python files in the `project_dir/code/` directory can be correctly imported by this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Get the current working directory\n",
        "notebooks_dir = os.getcwd()\n",
        "\n",
        "# Navigate up to the project directory\n",
        "project_dir = os.path.dirname(notebooks_dir)\n",
        "\n",
        "# Path to the `code` directory\n",
        "code_dir = os.path.join(project_dir, 'code')  \n",
        "\n",
        "# Add the code directory to the Python path\n",
        "if code_dir not in sys.path:\n",
        "    sys.path.insert(0, code_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ViBbqfx9un_"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from keras_cv.src.models.stable_diffusion.image_encoder import ImageEncoder\n",
        "from stable_diffusion import StableDiffusion\n",
        "from utils import process_image, augmenter\n",
        "\n",
        "from visualisation_utils import plot_attention_location, animate_locations\n",
        "from my_utils import dict_to_disk, dict_from_disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"GPUs available: \", tf.config.experimental.list_physical_devices('GPU'))\n",
        "device = tf.test.gpu_device_name()\n",
        "print(tf.test.gpu_device_name())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfCj5i1be890"
      },
      "source": [
        "# Initialize SD Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8b6beipfCA3"
      },
      "outputs": [],
      "source": [
        "# Inialize Stable Diffusion Model on GPU:0\n",
        "with tf.device(device):\n",
        "    image_encoder = ImageEncoder()\n",
        "    vae = tf.keras.Model(\n",
        "        image_encoder.input,\n",
        "        image_encoder.layers[-1].output,\n",
        "    )\n",
        "    model = StableDiffusion(img_width=512, img_height=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get latents for input image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = \"../images/polar_bear.jpg\"\n",
        "\n",
        "with tf.device(device):\n",
        "    input_image = process_image(image_path)\n",
        "    input_augmented = augmenter(input_image)\n",
        "    latent = vae(tf.expand_dims(input_augmented, axis=0), training=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Show the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_image = tf.cast(input_image, tf.int32)\n",
        "plt.imshow(input_image)\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run inference on input image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define context shape and generate random context variation\n",
        "context_shape = (1, 77, 768)\n",
        "# context = None\n",
        "context = tf.random.normal(context_shape)\n",
        "# context = tf.zeros((1, 77, 768))\n",
        "# context = tf.ones((1, 77, 768))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dictionary of structure { timestep : { resolution : self-attention map } }\n",
        "self_attn_dict:  Dict[int, Dict[int, np.ndarray]] = { }\n",
        "cross_attn_dict: Dict[int, Dict[int, np.ndarray]] = { }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_timesteps = 10\n",
        "\n",
        "for timestep in tqdm(np.arange(0, 1000, 1000 // num_timesteps)):\n",
        "    with tf.device(device):\n",
        "        self_attn_64,  self_attn_32,  self_attn_16,  self_attn_8, \\\n",
        "        cross_attn_64, cross_attn_32, cross_attn_16, cross_attn_8, \\\n",
        "        = model.generate_image(encoded_text=context, latent=latent, timestep=timestep)\n",
        "\n",
        "        # Average attention heads and store attention maps for current time step\n",
        "        self_attn_dict[timestep] = {\n",
        "            8:  self_attn_8.mean(axis=(0, 1)),\n",
        "            16: self_attn_16.mean(axis=(0, 1)),\n",
        "            32: self_attn_32.mean(axis=(0, 1)),\n",
        "            64: self_attn_64.mean(axis=(0, 1)),\n",
        "        }\n",
        "\n",
        "        cross_attn_dict[timestep] = {\n",
        "            8:  cross_attn_8.mean(axis=(0, 1)),\n",
        "            16: cross_attn_16.mean(axis=(0, 1)),\n",
        "            32: cross_attn_32.mean(axis=(0, 1)),\n",
        "            64: cross_attn_64.mean(axis=(0, 1)),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save self-attention maps to disk\n",
        "# dict_to_disk(\n",
        "#     self_attn_dict=self_attn_dict,\n",
        "#     filename=\"self_attn_maps/polar_bear\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualise the VAE latents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting the latents\n",
        "fig, axs = plt.subplots(1, 4, figsize=(20, 5))  # 1 row, 4 columns\n",
        "\n",
        "# Loop over each channel\n",
        "for i in range(4):\n",
        "    channel = latent[0, :, :, i]\n",
        "    axs[i].imshow(channel, cmap=\"gray\")\n",
        "    axs[i].set_title(f\"Channel {i+1}\")\n",
        "    axs[i].axis(\"off\")  # Hide axis\n",
        "\n",
        "fig.suptitle(\"Latents\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Show the self-attention maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change this to a value between 0 and 4095\n",
        "channel64_idx = 1140\n",
        "t = 900\n",
        "\n",
        "# Change `interpolate` to `False` to see raw pixel data\n",
        "plot_attention_location(\n",
        "    self_attn_dict[t],\n",
        "    orig_channel_idx=channel64_idx,\n",
        "    orig_res=64,\n",
        "    interpolate=False,\n",
        "    timestep=t\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Show the cross-attention maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show which time steps `t` are contained in the dictionary\n",
        "cross_attn_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = 64\n",
        "t = 900\n",
        "\n",
        "# for t in tqdm(cross_attn_dict.keys()):\n",
        "cross_attn = cross_attn_dict[t][res].reshape(res, res, 77)\n",
        "\n",
        "fig, axs = plt.subplots(9, 9, figsize=(10, 10))\n",
        "axs = axs.flatten()\n",
        "\n",
        "for idx, ax in enumerate(axs):\n",
        "    if idx < 77:\n",
        "        ax.imshow(cross_attn[:, :, idx])\n",
        "    ax.axis('off')\n",
        "\n",
        "fig.suptitle(f\"({res} x {res}) cross-attention for t = {t} {'(unconditional)' if context is None else ''} \", fontsize=24, y=0.99)\n",
        "fig.tight_layout()\n",
        "\n",
        "# Save figure to disk as PNG\n",
        "# fig.savefig(\n",
        "#     f\"bear_cross_res64_t{t}.png\",\n",
        "#     dpi=300,\n",
        "#     bbox_inches=\"tight\",\n",
        "#     pad_inches=0.1,\n",
        "# );"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "r52zlVDUJl_9",
        "8s6-2GsMiEwi",
        "YqhfoJBLPDkj",
        "HkDuWW_S6mFK",
        "Ir_E8vlvmC3i",
        "Jq_UYaw9z9W1",
        "AeVPOortS0Xg",
        "drmyt0Zq-Yuf",
        "F_2rE9z3lJxp",
        "gwq2tPWBoUCs",
        "ZuURD2pAgE6S",
        "MhoxFI32WrsZ",
        "KfCj5i1be890",
        "i9M17URAGrRT",
        "ZMg1NFyON8ve",
        "2uO8K_INONGB",
        "n4Mmj0o4OYWu",
        "25FBAbAWOjQT",
        "zdgg-Yirfj1s",
        "aiNXdN1UckMN",
        "XBW0qHSxlF0Z",
        "G4ktTGCsHjZU",
        "JTOmRatCdsap",
        "mc1OqXkMsik4",
        "EVTDEno2spnw"
      ],
      "last_runtime": {
        "build_target": "//experimental/humaninterface/explorations/diffseg/colab_runtime:ml_notebook",
        "kind": "private"
      },
      "name": "diffusion_inference_coco_cityscapes.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_coco.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1689088568669
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_coco.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1688922575491
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_coco.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1688833060440
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687892328479
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687837396445
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687364924074
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687114629456
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1686761132459
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1686163689880
        },
        {
          "file_id": "1z1ERc9A5S1u5ci0dHLSoqDgPeXpUI_lY",
          "timestamp": 1686159970456
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
