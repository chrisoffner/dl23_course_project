{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Internal imports   \n",
        "The following cell ensures that the Python files in the `project_dir/code/` directory can be correctly imported by this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add the project's `code` directory to the Python path\n",
        "notebooks_dir = os.getcwd()\n",
        "project_dir = os.path.dirname(notebooks_dir)\n",
        "code_dir = os.path.join(project_dir, 'code')  \n",
        "if code_dir not in sys.path:\n",
        "    sys.path.insert(0, code_dir)\n",
        "\n",
        "from my_utils import dict_from_disk, load_image_as_tensor\n",
        "\n",
        "\n",
        "from visualisation_utils import plot_masks_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    DEVICE = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "if DEVICE.type == 'cuda':\n",
        "    torch.cuda.manual_seed(0)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load cross-attention maps from disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = Path(r\"C:\\Users\\aapolina\\CODE\\diffusion_segmentation\\DATA\\data\\data\\ECSSD_resized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the path to the directory containing the cross-attention maps\n",
        "FEATURE_DIR = Path(f\"{DATA_PATH}/features/cross_attn_cpu\")\n",
        "\n",
        "# Filter files in directory for the cross-attention maps\n",
        "cross_attn_filenames = sorted([f for f in FEATURE_DIR.glob(\"*.h5\") if f.stem.endswith(\"_cross\")])\n",
        "\n",
        "# Load the cross-attention maps\n",
        "#list of 1000 samples, for each sample, first a dict with timestep keys, then for each timestep a dict with resolution keys\n",
        "cross_attn_maps = [dict_from_disk(str(f)) for f in tqdm(cross_attn_filenames)] \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cross_attn_maps[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "GT_DIR = Path(f\"{DATA_PATH}/gt\")\n",
        "\n",
        "# Load the ground truth masks for the cross-attention maps as (64, 64) tensors\n",
        "base_names = map(lambda path: path.stem, cross_attn_filenames)\n",
        "gt_paths = sorted([GT_DIR / f\"{base_name.split('_')[0]}.png\" for base_name in base_names])\n",
        "gt_segmentations = [load_image_as_tensor(path, True) for path in gt_paths]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a dataset and data loader from cross-attention maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cross_attn_maps_train, cross_attn_maps_valid, gt_segmentations_train, gt_segmentations_valid = train_test_split(cross_attn_maps, gt_segmentations, test_size=0.2, random_state=0, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from cross_attention_dataset import CrossAttentionDataset\n",
        "\n",
        "dataset = CrossAttentionDataset(cross_attn_maps, gt_segmentations)\n",
        "\n",
        "batch_size = 16\n",
        "shuffle = False\n",
        "\n",
        "dataset_train = CrossAttentionDataset(cross_attn_maps_train, gt_segmentations_train)\n",
        "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "dataset_valid = CrossAttentionDataset(cross_attn_maps_valid, gt_segmentations_valid)\n",
        "data_loader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=shuffle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from probing_models import LinearProbe2\n",
        "\n",
        "model = LinearProbe2().to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "criterion = torch.nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "count_parameters(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def store_weights(model):\n",
        "    weights = []\n",
        "    [weights.extend(v) for k,v in model.state_dict().items()]\n",
        "    weights = [w.detach().cpu().numpy().tolist() for w in weights]\n",
        "    return weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def store_weights_epoch(model):\n",
        "    return {'wt': model.ts_weights.detach().cpu().numpy().tolist(), \n",
        "            'wc':model.ch_weights.detach().cpu().numpy().tolist(), \n",
        "            'wr':model.res_weights.detach().cpu().numpy().tolist(), \n",
        "            'ws':model.scale_weights.detach().cpu().numpy().tolist()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def L1_penalty(model):\n",
        "    L1 = 0\n",
        "    for p in model.parameters():\n",
        "        L1 = L1 + p.abs().sum()\n",
        "    #return torch.sum(torch.abs(model.ts_weights)) + torch.sum(torch.abs(model.ch_weights)) + torch.sum(torch.abs(model.res_weights)) + torch.sum(torch.abs(model.scale_weights))\n",
        "    return L1\n",
        "\n",
        "L1_penalty(model)\n",
        "\n",
        "\n",
        "lambda_L1 = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def L2_penalty(model):\n",
        "    L2 = 0\n",
        "    for p in model.parameters():\n",
        "        L2 = L2 + p.pow(2).sum()\n",
        "    return L2\n",
        "\n",
        "L2_penalty(model)\n",
        "\n",
        "lambda_L2 = 1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_training(model,loader, criterion):\n",
        "    loss_valid = []\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for  cross_attn_8, cross_attn_16, cross_attn_32, cross_attn_64, gt in loader:\n",
        "            cross_attn_8 = cross_attn_8.squeeze().to(DEVICE)\n",
        "            cross_attn_16 = cross_attn_16.squeeze().to(DEVICE)\n",
        "            cross_attn_32 = cross_attn_32.squeeze().to(DEVICE)\n",
        "            cross_attn_64 = cross_attn_64.squeeze().to(DEVICE)\n",
        "            gt = gt.to(DEVICE)\n",
        "            output = model([cross_attn_8, cross_attn_16, cross_attn_32, cross_attn_64])\n",
        "\n",
        "\n",
        "            loss = criterion(output, gt)\n",
        "            #loss += lambda_L1 * L1_penalty(model)\n",
        "            loss_valid.append(loss.item())\n",
        "\n",
        "    return loss_valid #np.mean(loss_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "losses_train = []\n",
        "losses_valid = []\n",
        "\n",
        "losses_train_all = []\n",
        "losses_valid_all = []\n",
        "\n",
        "all_weights = {}\n",
        "\n",
        "for epoch in range(30):\n",
        "    loss_epoch_train=[]\n",
        "\n",
        "    with tqdm(data_loader_train, desc=f\"Epoch {epoch}\") as tepoch:\n",
        "\n",
        "        for cross_attn_8, cross_attn_16, cross_attn_32, cross_attn_64, gt in tepoch:\n",
        "            cross_attn_8 = cross_attn_8.squeeze().to(DEVICE)\n",
        "            cross_attn_16 = cross_attn_16.squeeze().to(DEVICE)\n",
        "            cross_attn_32 = cross_attn_32.squeeze().to(DEVICE)\n",
        "            cross_attn_64 = cross_attn_64.squeeze().to(DEVICE)\n",
        "            gt = gt.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model([cross_attn_8, cross_attn_16, cross_attn_32, cross_attn_64])\n",
        "            loss = criterion(output, gt)\n",
        "\n",
        "            #loss += lambda_L1 * L1_penalty(model)\n",
        "\n",
        "            loss_epoch_train.append(loss.item())\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the progress bar description\n",
        "            tepoch.set_description(f\"Epoch {epoch}\")# Loss: {loss.item():.4f}\")\n",
        "\n",
        "    all_weights[epoch] = store_weights_epoch(model)\n",
        "    \n",
        "\n",
        "    loss_epoch_valid = evaluate_training(model, data_loader_valid, criterion)\n",
        "\n",
        "    losses_train_all.append(loss_epoch_train) \n",
        "    losses_valid_all.append(loss_epoch_valid)\n",
        "       \n",
        "    loss_epoch_train = np.mean(loss_epoch_train)\n",
        "    loss_epoch_valid = np.mean(loss_epoch_valid)\n",
        "\n",
        "    losses_train.append(loss_epoch_train)\n",
        "    losses_valid.append(loss_epoch_valid)  \n",
        "\n",
        "    print(f\"Epoch {epoch} \\n Train Loss: {loss_epoch_train:.4f} \\n Valid Loss: {loss_epoch_valid:.4f}\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize the evolution of the weights during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for w in ['wt']:#['wt', 'wc', 'wr', 'ws']:\n",
        "    n = len(all_weights[0][w])\n",
        "    if w=='wt':\n",
        "        suffix = [0, 111, 222, 333, 444, 555, 666, 777, 888, 999]\n",
        "    elif w=='wc':\n",
        "        suffix = range(77)\n",
        "    elif w=='wr':\n",
        "        suffix = [8,16,32,64]\n",
        "    elif w=='ws':\n",
        "        suffix = [\"\"]   \n",
        "    assert n==len(suffix)\n",
        "    colors = plt.cm.PiYG(np.linspace(0,1,n))  \n",
        "                        \n",
        "    for i in range(n):\n",
        "        data = [all_weights[epoch][w][i] for epoch in all_weights.keys()]\n",
        "        plt.plot(data, label = f\"{w}_{suffix[i]}\", color=colors[i])\n",
        "\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Weight')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# df = pd.DataFrame(data=all_weights[0])\n",
        "# df.T.plot(legend=False, figsize=(20,15))\n",
        "\n",
        "# new_weights = all_weights[0][0]\n",
        "# old_weights = all_weights[0][0]\n",
        "\n",
        "def plot_weights(old_weights, new_weights):\n",
        "\n",
        "    # Plot the old_weights\n",
        "    plt.plot(old_weights, label='Old Weights')\n",
        "\n",
        "    # Plot the new_weights\n",
        "    plt.plot(new_weights, label='New Weights')\n",
        "\n",
        "    # Add labels and legend\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Weight')\n",
        "    plt.legend()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Train and Validation losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(1, len(losses_train) + 1)\n",
        "\n",
        "\n",
        "plt.plot(epochs, losses_train, 'b', label='Train Loss Mean')\n",
        "plt.plot(epochs, losses_valid, 'r', label='Validation Loss Mean')\n",
        "\n",
        "# add min loss lines\n",
        "lt_min = min(losses_train)\n",
        "plt.axhline(lt_min, color='blue', linestyle='--',lw=1)\n",
        "plt.annotate(text=f\"{lt_min:.3f}\",xy=(0,lt_min+0.001),color='blue'    )\n",
        "lv_min = min(losses_valid)  \n",
        "plt.axhline(lv_min, color='red', linestyle='--',lw=1)\n",
        "plt.annotate(text=f\"{lv_min:.3f}\",xy=(0,lv_min+0.001),color='red'    )\n",
        "\n",
        "plt.title('Train and Validation Losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train = np.array(losses_train_all)\n",
        "y_valid = np.array(losses_valid_all)\n",
        "\n",
        "x = list(range(len(y_train)))\n",
        "y_train_mean = np.mean(y_train, axis=1)\n",
        "y_train_min = np.min(y_train, axis=1)\n",
        "y_train_max = np.max(y_train, axis=1)\n",
        "y_valid_mean = np.mean(y_valid, axis=1)\n",
        "y_valid_min = np.min(y_valid, axis=1)\n",
        "y_valid_max = np.max(y_valid, axis=1)\n",
        "\n",
        "# Plot the line on the mean and shaded area of the range for training and validation losses\n",
        "plt.plot(x, y_train_mean, color='blue', label='Train Mean')\n",
        "plt.fill_between(x, y_train_min, y_train_max, color='lightblue', alpha=0.5, label='Train Range')\n",
        "\n",
        "plt.plot(x, y_valid_mean, color='red', label='Valid Mean')\n",
        "plt.fill_between(x, y_valid_min, y_valid_max, color='pink', alpha=0.5, label='Valid Range')\n",
        "\n",
        "\n",
        "# add min loss lines\n",
        "lt_min = min(losses_train)\n",
        "plt.axhline(lt_min, color='blue', linestyle='--',lw=1)\n",
        "plt.annotate(text=f\"{lt_min:.3f}\",xy=(0,lt_min+0.001),color='blue'    )\n",
        "\n",
        "lv_min = min(losses_valid)  \n",
        "plt.axhline(lv_min, color='red', linestyle='--',lw=1)\n",
        "plt.annotate(text=f\"{lv_min:.3f}\",xy=(0,lv_min+0.001),color='red'    )\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('BCE Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save/Load the trained model to/from checkpoint\n",
        "\n",
        "(saving the last state for now)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_path = os.path.join(os.getcwd(), 'cross_attn_experiment.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load model from checkpoint\n",
        "model = LinearProbe2(n_timesteps=len(dataset_train.timesteps)).to(DEVICE)\n",
        "model.load_state_dict(torch.load(checkpoint_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize the results \n",
        "### predicted mask vs. ground truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.to(DEVICE)\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_img_output_gt(indices):\n",
        "\n",
        "    fig, axs = plt.subplots(3, len(indices), figsize=(15, 5))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        img = load_image_as_tensor(gt_paths[idx].parent.parent / \"img\" / (gt_paths[idx].stem + \".jpg\"))\n",
        "        gt = load_image_as_tensor(gt_paths[idx], True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            cross_attn_8, cross_attn_16, cross_attn_32, cross_attn_64, gt = dataset[idx]\n",
        "            output = model(cross_attn_8.to(DEVICE), cross_attn_16.to(DEVICE),cross_attn_32.to(DEVICE), cross_attn_64.to(DEVICE))\n",
        "            \n",
        "            loss = criterion(output, gt.to(DEVICE)).cpu()\n",
        "            output = output.squeeze().detach().cpu()\n",
        "\n",
        "        axs[0, i].imshow(img.permute(1, 2, 0))\n",
        "        axs[0, i].set_title(f\"Image {idx}\")\n",
        "\n",
        "        axs[1, i].imshow(output)\n",
        "        axs[1, i].set_title(f\"Output {idx}: {loss:.4f}\")\n",
        "\n",
        "        axs[2, i].imshow(gt)\n",
        "        axs[2, i].set_title(f\"Ground Truth {idx}: {loss:.4f}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def image_overlay_error(test_img, gt_img):\n",
        "\n",
        "    image_overlay_error = np.zeros((test_img.shape[0], test_img.shape[1], 3))\n",
        "\n",
        "    image_overlay_error[(test_img == 1) &  (gt_img == 1)] = [1,1,1]         # correct as 1 - white\n",
        "    image_overlay_error[(test_img == 0) &  (gt_img == 0)] = [0,0,0]         # correct as 0 - black \n",
        "    image_overlay_error[(test_img == 1) &  (gt_img == 0)] = [1, 0.65, 0]     # false positive (mislabelled background as foreground) - orange\n",
        "    image_overlay_error[(test_img == 0) &  (gt_img == 1)] = [0, 0.55,0.7]    # false negative (mislabelled foreground as background) - blue\n",
        "\n",
        "    return image_overlay_error\n",
        "\n",
        "def img_binary(img):\n",
        "    return (img >= 0.5).float()\n",
        "\n",
        "def plot_segmentation_results(idx, img, output, gt, loss, save=False):\n",
        "\n",
        "    fig, axs = plt.subplots(1, 5, figsize=(15, 5))\n",
        "\n",
        "\n",
        "    gt = img_binary(gt)\n",
        "    output_binary = img_binary(output)\n",
        "\n",
        "    diff = image_overlay_error(output_binary, gt)\n",
        "\n",
        "    axs[0].imshow(img.permute(1, 2, 0))\n",
        "    axs[1].imshow(output, cmap='bone')\n",
        "    axs[2].imshow(output_binary, cmap='bone')\n",
        "    axs[3].imshow(diff )\n",
        "    axs[4].imshow(gt, cmap='gray')\n",
        "\n",
        "    axs[0].set_title(f'Image {idx}')\n",
        "    axs[1].set_title(f'Output {loss:.3f}')\n",
        "    axs[2].set_title('Output Binary')\n",
        "    axs[3].set_title('Difference')\n",
        "    axs[4].set_title('Ground Truth')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        path = r\"C:\\Users\\aapolina\\CODE\\diffusion_segmentation\\output\\231221_cross_attn_upsampled\\diff\"\n",
        "        filename = f'img_{idx}.png'\n",
        "        plt.savefig(os.path.join(path, filename))   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def results_one_sample(idx):\n",
        "\n",
        "    img = load_image_as_tensor(gt_paths[idx].parent.parent / \"img\" / (gt_paths[idx].stem + \".jpg\"))\n",
        "    gt = load_image_as_tensor(gt_paths[idx], True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        cross_attn_8, cross_attn_16, cross_attn_32, cross_attn_64, _ = dataset[idx]\n",
        "        cross_attn_8 = cross_attn_8.squeeze().to(DEVICE).unsqueeze(0)\n",
        "        cross_attn_16 = cross_attn_16.squeeze().to(DEVICE).unsqueeze(0)\n",
        "        cross_attn_32 = cross_attn_32.squeeze().to(DEVICE).unsqueeze(0)\n",
        "        cross_attn_64 = cross_attn_64.squeeze().to(DEVICE).unsqueeze(0)\n",
        "        gt = gt.to(DEVICE)\n",
        "        output = model([cross_attn_8, cross_attn_16, cross_attn_32, cross_attn_64]).squeeze(0)\n",
        "        loss = criterion(output, gt)\n",
        "    \n",
        "    output = output.cpu()\n",
        "    gt = gt.cpu()\n",
        "    loss = loss.item()\n",
        "\n",
        "    return img, output, gt, loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "idx = 111\n",
        "img, output, gt, loss = results_one_sample(idx)\n",
        "plot_segmentation_results(idx, img, output, gt, loss, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Calculate scores/metrics\n",
        "for the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def IoU(prediction, groundtruth):\n",
        "    intersection = np.logical_and(groundtruth, prediction)\n",
        "    union = np.logical_or(groundtruth, prediction)\n",
        "    iou_score = np.sum(intersection) / np.sum(union)\n",
        "    return iou_score\n",
        "\n",
        "def dice_coefficient(prediction, groundtruth):\n",
        "    intersection = np.logical_and(groundtruth, prediction)\n",
        "    dsc = (2 * np.sum(intersection)) / (np.sum(groundtruth) + np.sum(prediction))\n",
        "    return dsc\n",
        "\n",
        "def precision(prediction, groundtruth):\n",
        "    intersection = np.logical_and(groundtruth, prediction)\n",
        "    prec = np.sum(intersection) / np.sum(prediction)\n",
        "    return prec\n",
        "\n",
        "def recall(prediction, groundtruth):\n",
        "    intersection = np.logical_and(groundtruth, prediction)\n",
        "    rec = np.sum(intersection) / np.sum(groundtruth)\n",
        "    return rec\n",
        "\n",
        "def accuracy(prediction, groundtruth):\n",
        "    acc = np.sum(prediction == groundtruth) / np.prod(groundtruth.shape)\n",
        "    return acc\n",
        "\n",
        "def F1_score(prediction, groundtruth):\n",
        "    prec = precision(prediction, groundtruth)\n",
        "    rec = recall(prediction, groundtruth)\n",
        "    f1 = 2 * (prec * rec) / (prec + rec)\n",
        "    return f1\n",
        "\n",
        "def performance_metrics(prediction, groundtruth):\n",
        "    iou = IoU(prediction, groundtruth)\n",
        "    dsc = dice_coefficient(prediction, groundtruth)\n",
        "    prec = precision(prediction, groundtruth)\n",
        "    rec = recall(prediction, groundtruth)\n",
        "    acc = accuracy(prediction, groundtruth)\n",
        "    f1 = F1_score(prediction, groundtruth)\n",
        "    return iou, dsc, prec, rec, acc, f1\n",
        "\n",
        "def mask_pecentage(mask):\n",
        "    return np.sum(mask) / np.prod(mask.shape)\n",
        "\n",
        "\n",
        "output_np = img_binary(output).numpy()\n",
        "gt_np = gt.numpy() \n",
        "iou, dsc, prec, rec, acc, f1 = performance_metrics(output_np, gt_np) \n",
        "iou, dsc, prec, rec, acc, f1, loss, mask_pecentage(gt_np)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_results={'iou':[], 'dice':[], 'loss':[], 'mask_perc':[]}\n",
        "\n",
        "for idx in range(1000):\n",
        "    img, output, gt, loss = results_one_sample(idx)\n",
        "    output_np = img_binary(output).numpy()\n",
        "    gt_np = gt.numpy() \n",
        "    all_results['iou'].append(IoU(output_np, gt_np))\n",
        "    all_results['dice'].append(dice_coefficient(output_np, gt_np))  \n",
        "    all_results['loss'].append(loss)\n",
        "    all_results['mask_perc'].append(mask_pecentage(gt_np))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_scatter_results(all_results, sx = \"mask_perc\", sy = \"iou\"):\n",
        "    labels = {'iou': \"IoU\", 'dice': \"Dice\", 'loss': \"Loss\", 'mask_perc': \"Mask Percentage\"}\n",
        "\n",
        "    \n",
        "    # Scatter plot\n",
        "    x = all_results[sx]\n",
        "    y = all_results[sy]\n",
        "    plt.scatter(x,y, color='red', marker='o', s=1)\n",
        "\n",
        "    # Add dashed lines for mean x and mean y\n",
        "    mean_x = np.mean(x)\n",
        "    mean_y = np.mean(y)\n",
        "\n",
        "    plt.axvline(mean_x,linestyle='--', lw=1,  color='red')\n",
        "    plt.axhline(mean_y, linestyle='--', lw=1, color='red')\n",
        "\n",
        "    plt.annotate(text=f\"Mean: {mean_x:.2f}\", xy=(mean_x, 0), color='red')\n",
        "    plt.annotate(text=f\"Mean: {mean_y:.2f}\", xy=(0, mean_y), color='red')\n",
        "\n",
        "\n",
        "    #add trendline to plot\n",
        "    # z = np.polyfit(x,y,deg=1)\n",
        "    # p = np.poly1d(z)\n",
        "    # plt.plot(x, p(x), color='red', linestyle=':', lw=0.5)\n",
        "\n",
        "\n",
        "    # Set labels and title\n",
        "    plt.xlabel(labels[sx])\n",
        "    plt.ylabel(labels[sy])\n",
        "    plt.title(f\"All samples results ({labels[sx]} vs {labels[sy]})\")\n",
        "\n",
        "    # Show the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_scatter_results(all_results, sx = \"mask_perc\", sy = \"iou\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_scatter_results(all_results, sx = \"mask_perc\", sy = \"dice\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_scatter_results(all_results, sx = \"loss\", sy = \"iou\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_scatter_results(all_results, sx = \"mask_perc\", sy = \"loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_loss = min(all_results['loss'])\n",
        "idx = all_results['loss'].index(best_loss)\n",
        "\n",
        "plot_segmentation_results( idx, *results_one_sample(idx), False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "worst_loss = max(all_results['loss'])\n",
        "idx = all_results['loss'].index(worst_loss)\n",
        "\n",
        "plot_segmentation_results( idx, *results_one_sample(idx), False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_iou = max(all_results['iou'])\n",
        "idx = all_results['iou'].index(best_iou)\n",
        "plot_segmentation_results( idx, *results_one_sample(idx), False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "worst_iou = min(all_results['iou'])\n",
        "idx = all_results['iou'].index(worst_iou)\n",
        "plot_segmentation_results( idx, *results_one_sample(idx), False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "r52zlVDUJl_9",
        "8s6-2GsMiEwi",
        "YqhfoJBLPDkj",
        "HkDuWW_S6mFK",
        "Ir_E8vlvmC3i",
        "Jq_UYaw9z9W1",
        "AeVPOortS0Xg",
        "drmyt0Zq-Yuf",
        "F_2rE9z3lJxp",
        "gwq2tPWBoUCs",
        "ZuURD2pAgE6S",
        "MhoxFI32WrsZ",
        "KfCj5i1be890",
        "i9M17URAGrRT",
        "ZMg1NFyON8ve",
        "2uO8K_INONGB",
        "n4Mmj0o4OYWu",
        "25FBAbAWOjQT",
        "zdgg-Yirfj1s",
        "aiNXdN1UckMN",
        "XBW0qHSxlF0Z",
        "G4ktTGCsHjZU",
        "JTOmRatCdsap",
        "mc1OqXkMsik4",
        "EVTDEno2spnw"
      ],
      "last_runtime": {
        "build_target": "//experimental/humaninterface/explorations/diffseg/colab_runtime:ml_notebook",
        "kind": "private"
      },
      "name": "diffusion_inference_coco_cityscapes.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_coco.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1689088568669
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_coco.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1688922575491
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_coco.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1688833060440
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687892328479
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687837396445
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687364924074
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687114629456
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1686761132459
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1686163689880
        },
        {
          "file_id": "1z1ERc9A5S1u5ci0dHLSoqDgPeXpUI_lY",
          "timestamp": 1686159970456
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
