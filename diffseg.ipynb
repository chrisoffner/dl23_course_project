{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instructions\n",
        "Please run the following cells sequentially\n",
        "1. Initialize SD Model\n",
        "2. Add your own image and update ``image_path`` variable. \n",
        "3. Feel free to play with DiffSeg hyper-parameters such as the ``KL_THRESHOLD``."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r52zlVDUJl_9"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ViBbqfx9un_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras_cv.src.models.stable_diffusion.image_encoder import ImageEncoder\n",
        "from diffusion_models.stable_diffusion import StableDiffusion\n",
        "from utils import process_image, augmenter, vis_without_label\n",
        "from diffseg.segmentor import DiffSeg\n",
        "\n",
        "from visualisation_utils import plot_attention, animate_attention, convert_channel_idx\n",
        "\n",
        "# !nvidia-smi # Uncomment if you have an NVIDIA GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"GPUs available: \", tf.config.experimental.list_physical_devices('GPU'))\n",
        "device = tf.test.gpu_device_name()\n",
        "print(tf.test.gpu_device_name())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfCj5i1be890"
      },
      "source": [
        "# 1. Initialize SD Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8b6beipfCA3"
      },
      "outputs": [],
      "source": [
        "# Inialize Stable Diffusion Model on GPU:0 \n",
        "with tf.device('/GPU:0'):\n",
        "  image_encoder = ImageEncoder()\n",
        "  vae=tf.keras.Model(\n",
        "            image_encoder.input,\n",
        "            image_encoder.layers[-1].output,\n",
        "        )\n",
        "  model = StableDiffusion(img_width=512, img_height=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Run Inference on Real Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The first time running this cell will be slow because the model needs to download and loads pre-trained weights.\n",
        "\n",
<<<<<<< Updated upstream
        "image_path = \"./images/img1.jpeg\" # Specify the path to your image\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "  images = process_image(image_path)\n",
        "  images = augmenter(images)\n",
        "  latent = vae(tf.expand_dims(images, axis=0), training=False)\n",
        "  images, weight_64, weight_32, weight_16, weight_8, _, _, _, _ = model.text_to_image(\n",
        "    prompt=None,\n",
        "    batch_size=1,\n",
        "    latent=latent,\n",
        "    timestep=300\n",
        "  )\n",
=======
        "#image_path = \"./images/img1.jpeg\"  # Specify the path to your image\n",
        "image_path = r\"C:\\Users\\aapolina\\CAS\\CAS_CODE\\diffusion_segmentation\\images\\549ac0a17a0f4c868aaa23c38b5e17dd.jpeg\"\n",
        "# image_path = r\"C:\\Users\\aapolina\\CAS\\CAS_CODE\\diffusion_segmentation\\images\\ae81504131944b3183fcf289330f6aae.jpeg\"\n",
        "timestep = 300\n",
        "\n",
        "with tf.device(device):\n",
        "    images = process_image(image_path)\n",
        "    images = augmenter(images)\n",
        "    latent = vae(tf.expand_dims(images, axis=0), training=False)\n",
        "    images, weight_64, weight_32, weight_16, weight_8, _, _, _, _ = model.text_to_image(\n",
        "        batch_size=1,\n",
        "        latent=latent,\n",
        "        timestep=timestep\n",
        "    )\n",
>>>>>>> Stashed changes
        "\n",
        "# Store self-attention maps in a dictionary for resolution-specific access\n",
        "res2weights = {\n",
        "    8:  weight_8,\n",
        "    16: weight_16,\n",
        "    32: weight_32,\n",
        "    64: weight_64\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Show the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.imshow(images[0])\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualise the VAE latents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting the latents\n",
        "fig, axs = plt.subplots(1, 4, figsize=(20, 5))  # 1 row, 4 columns\n",
        "\n",
        "# Loop over each channel\n",
        "for i in range(4):\n",
        "    channel = latent[0, :, :, i]\n",
        "    axs[i].imshow(channel, cmap='gray')\n",
        "    axs[i].set_title(f'Channel {i+1}')\n",
        "    axs[i].axis('off')  # Hide axis\n",
        "\n",
        "fig.suptitle(\"Latents\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot self-attention map of resolution `res`\n",
        "\n",
        "Now we compare two ways of visualising the same self-attention map by reshaping a self-attention tensor `weight_N` (for `N` $\\in \\{8, 16, 32, 64\\}$) in two different ways.\n",
        "\n",
        "As an example, take a $64 \\times 64$ attention map with original tensor shape $(1, 8, 4096, 4096):$\n",
        "\n",
        "Interpretation $A$ reshapes the $(1, 8, 4096, 4096)$ tensor to $(1, 8, 4096, 64,   64),$ then sums across the $8$ attention heads. The resulting shape of $A$ is $(4096, 64,   64).$\n",
        "\n",
        "Interpretation $B$ reshapes the $(1, 8, 4096, 4096)$ tensor to $(1, 8, 64,  64, 4096),$ then sums across the $8$ attention heads. The resulting shape of $B$ is $(64,   64, 4096).$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = 32\n",
        "\n",
<<<<<<< Updated upstream
        "# Change this to a value between 0 and 4095\n",
        "channel64 = 2355\n",
        "channel = convert_channel_idx(orig_channel=channel64, orig_res=64, new_res=res)\n",
        "\n",
        "print(f\"Showing channel {channel} that corresponds to channel {channel64} in the \" \\\n",
        "       \"64 x 64 self-attention map.\")\n",
        "\n",
        "plot_attention(res2weights, res, channel)"
=======
        "# Change `interpolate` to `False` to see raw pixel data\n",
        "plot_attention_location(\n",
        "    res2weights,\n",
        "    orig_channel_idx=channel64_idx,\n",
        "    orig_res=64,\n",
        "    interpolate=False,\n",
        "    timestep=timestep\n",
        ")"
>>>>>>> Stashed changes
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following cell renders an animation for each resolution. This can take a long time for high `num_frames`.\n",
        "\n",
        "Make sure that `num_frames` $\\le$ `res`$^2.$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "animate_attention(res2weights, res=8,  num_frames=8**2, fps=1)\n",
        "# animate_attention(res2weights, res=16, num_frames=16**2, fps=1)\n",
        "# animate_attention(res2weights, res=32, num_frames=num_frames, fps=1)\n",
        "# animate_attention(res2weights, res=64, num_frames=num_frames, fps=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Generate Segmentation Masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vis_anchor_points(image, num_anchor_points):    \n",
        "  # add locations of the anchor points \n",
        "  w,h,_ = image.shape\n",
        "  dw = w/num_anchor_points\n",
        "  dh = h/num_anchor_points\n",
        "  for i in range(num_anchor_points):\n",
        "    for j in range(num_anchor_points):\n",
        "      plt.plot(dw*0.5 + dw*i, dh*0.5 +dh*j, marker='o', color=\"red\", markersize = 2) \n",
        "\n",
        "  plt.imshow(image) \n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "KL_THRESHOLD = [0.00001]*3 # KL_THRESHOLD controls the merging threshold\n",
        "NUM_POINTS = 4\n",
        "REFINEMENT = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image = images[0]\n",
        "image.shape #512 x 512 x 3(rgb) \n",
        "vis_anchor_points(image,NUM_POINTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "with tf.device('/GPU:0'):\n",
        "  segmentor = DiffSeg(KL_THRESHOLD, REFINEMENT, NUM_POINTS)\n",
        "  pred = segmentor.segment(weight_64, weight_32, weight_16, weight_8)[0] # b x 512 x 512 - for each pixel an index of the segment\n",
        "  n_segments = len(set(pred.flatten()))\n",
        "  print(f\"predicted {n_segments} segments\")\n",
        "  vis_without_label(pred, image, num_class=n_segments, num_anchor_points=NUM_POINTS)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "r52zlVDUJl_9",
        "8s6-2GsMiEwi",
        "YqhfoJBLPDkj",
        "HkDuWW_S6mFK",
        "Ir_E8vlvmC3i",
        "Jq_UYaw9z9W1",
        "AeVPOortS0Xg",
        "drmyt0Zq-Yuf",
        "F_2rE9z3lJxp",
        "gwq2tPWBoUCs",
        "ZuURD2pAgE6S",
        "MhoxFI32WrsZ",
        "KfCj5i1be890",
        "i9M17URAGrRT",
        "ZMg1NFyON8ve",
        "2uO8K_INONGB",
        "n4Mmj0o4OYWu",
        "25FBAbAWOjQT",
        "zdgg-Yirfj1s",
        "aiNXdN1UckMN",
        "XBW0qHSxlF0Z",
        "G4ktTGCsHjZU",
        "JTOmRatCdsap",
        "mc1OqXkMsik4",
        "EVTDEno2spnw"
      ],
      "last_runtime": {
        "build_target": "//experimental/humaninterface/explorations/diffseg/colab_runtime:ml_notebook",
        "kind": "private"
      },
      "name": "diffusion_inference_coco_cityscapes.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_coco.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1689088568669
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_coco.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1688922575491
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_coco.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1688833060440
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687892328479
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687837396445
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687364924074
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687114629456
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1686761132459
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1686163689880
        },
        {
          "file_id": "1z1ERc9A5S1u5ci0dHLSoqDgPeXpUI_lY",
          "timestamp": 1686159970456
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
