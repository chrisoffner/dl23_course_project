{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ViBbqfx9un_"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras_cv.src.models.stable_diffusion.image_encoder import ImageEncoder\n",
        "from diffusion_models.stable_diffusion import StableDiffusion\n",
        "from utils import process_image, augmenter\n",
        "\n",
        "from visualisation_utils import plot_attention_location, animate_locations\n",
        "from my_utils import dict_to_disk, dict_from_disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"GPUs available: \", tf.config.experimental.list_physical_devices('GPU'))\n",
        "device = tf.test.gpu_device_name()\n",
        "print(tf.test.gpu_device_name())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfCj5i1be890"
      },
      "source": [
        "# Initialize SD Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8b6beipfCA3"
      },
      "outputs": [],
      "source": [
        "# Inialize Stable Diffusion Model on GPU:0\n",
        "with tf.device(device):\n",
        "    image_encoder = ImageEncoder()\n",
        "    vae = tf.keras.Model(\n",
        "        image_encoder.input,\n",
        "        image_encoder.layers[-1].output,\n",
        "    )\n",
        "    model = StableDiffusion(img_width=512, img_height=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run inference on real image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run image through VAE encoder\n",
        "image_path = \"./images/img2.jpeg\"\n",
        "\n",
        "with tf.device(device):\n",
        "    image = process_image(image_path)\n",
        "    image = augmenter(image)\n",
        "    latent = vae(tf.expand_dims(image, axis=0), training=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dictionary of structure { timestep : { resolution : self-attention map } }\n",
        "self_attn_dict: Dict[int, Dict[int, np.ndarray]] = { }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform one denoising step\n",
        "num_timesteps = 10\n",
        "\n",
        "for timestep in np.arange(0, 1000, 1000 // num_timesteps):\n",
        "    with tf.device(device):\n",
        "        weight_64, weight_32, weight_16, weight_8 = model.generate_image(\n",
        "            batch_size=1,\n",
        "            latent=latent,\n",
        "            timestep=timestep,\n",
        "        )\n",
        "\n",
        "        # Average over attention heads and store self-attention maps for\n",
        "        # current time step in dictionary\n",
        "        self_attn_dict[timestep] = {\n",
        "            8:  weight_8.mean(axis=(0,1)),\n",
        "            16: weight_16.mean(axis=(0,1)),\n",
        "            32: weight_32.mean(axis=(0,1)),\n",
        "            64: weight_64.mean(axis=(0,1))\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save self-attention maps to disk\n",
        "dict_to_disk(\n",
        "    self_attn_dict=self_attn_dict,\n",
        "    filename=\"self_attn_maps/car\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Show the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting the input and output image\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
        "axs[0].imshow(input_image[0])\n",
        "axs[1].imshow(output_image[0])\n",
        "\n",
        "axs[0].set_title(\"Input image\")\n",
        "axs[1].set_title(\"Output image\")\n",
        "\n",
        "axs[0].axis(\"off\")\n",
        "axs[1].axis(\"off\")\n",
        "\n",
        "fig.suptitle(f\"Time step {timestep}\", fontsize=24)\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualise the VAE latents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting the latents\n",
        "fig, axs = plt.subplots(1, 4, figsize=(20, 5))  # 1 row, 4 columns\n",
        "\n",
        "# Loop over each channel\n",
        "for i in range(4):\n",
        "    channel = latent[0, :, :, i]\n",
        "    axs[i].imshow(channel, cmap=\"gray\")\n",
        "    axs[i].set_title(f\"Channel {i+1}\")\n",
        "    axs[i].axis(\"off\")  # Hide axis\n",
        "\n",
        "fig.suptitle(\"Latents\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change this to a value between 0 and 4095\n",
        "channel64_idx = 1110\n",
        "\n",
        "# Change `interpolate` to `False` to see raw pixel data\n",
        "plot_attention_location(\n",
        "    self_attn_dict[0],\n",
        "    orig_channel_idx=channel64_idx,\n",
        "    orig_res=64,\n",
        "    interpolate=False,\n",
        "    timestep=timestep\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following cell renders the previous $2 \\times 4$ plot as an animation that iterates over each pixel in the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to render animation. This can take a while.\n",
        "# animate_locations(res2weights, num_frames=64**2, fps=15, interpolate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "def render_attention_animation(\n",
        "        timestep_range,\n",
        "        orig_channel_idx=2355,\n",
        "        orig_res=64,\n",
        "        interpolate=False,\n",
        "        save_path='attention_animation.mp4',\n",
        "        fps: int = 5\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Renders an animation of attention maps over a range of timesteps.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    res2weights      : Dictionary containing { resolution: attention_map } pairs\n",
        "    timestep_range   : Tuple or list defining the start and end of the timestep range\n",
        "    orig_channel_idx : Channel index specifying a location in the orig_res map\n",
        "    orig_res         : Resolution of the attention map in which orig_channel_idx\n",
        "                       has been chosen\n",
        "    interpolate      : Boolean deciding whether to render with bicubic upscaling\n",
        "    save_path        : Path to save the animation\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
        "    title = plt.suptitle(t='', fontsize = 20)\n",
        "\n",
        "    def update_frame(timestep):\n",
        "        # Clear both subplots\n",
        "        axs[0, 0].cla()  \n",
        "        axs[1, 0].cla()\n",
        "\n",
        "        # Set title\n",
        "        title.set_text(f\"Self-attention maps for timestep {timestep}\")\n",
        "\n",
        "        image_path = \"./images/img1.jpeg\"  # Specify the path to your image\n",
        "\n",
        "        # Run inference to obtain self-attention maps for current time step\n",
        "        with tf.device(device):\n",
        "            images = process_image(image_path)\n",
        "            images = augmenter(images)\n",
        "            latent = vae(tf.expand_dims(images, axis=0), training=False)\n",
        "            _, _, weight_64, weight_32, weight_16, weight_8, _, _, _, _ = model.text_to_image(\n",
        "                batch_size=1,\n",
        "                latent=latent,\n",
        "                timestep=timestep\n",
        "            )\n",
        "\n",
        "        # Store self-attention maps in a dictionary for resolution-specific access\n",
        "        res2weights = { 8: weight_8, 16: weight_16, 32: weight_32, 64: weight_64 }\n",
        "\n",
        "        artists = plot_attention_location(\n",
        "            res2weights,\n",
        "            orig_channel_idx=orig_channel_idx,\n",
        "            orig_res=orig_res,\n",
        "            interpolate=interpolate,\n",
        "            timestep=timestep,\n",
        "            fig=fig,\n",
        "            axs=axs\n",
        "        )\n",
        "        return artists\n",
        "\n",
        "    ani = FuncAnimation(fig, update_frame, frames=timestep_range, blit=True)\n",
        "\n",
        "    # Show a progress bar while rendering the animation, then save file to disk\n",
        "    with tqdm(total=len(timestep_range), desc=\"Saving animation\") as pbar:\n",
        "        ani.save(\n",
        "            save_path,\n",
        "            writer='ffmpeg',\n",
        "            fps=fps,\n",
        "            progress_callback=lambda i, n: pbar.update()\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "render_attention_animation(\n",
        "    list(range(0, 1001, 5)),\n",
        "    orig_channel_idx=1404,\n",
        "    save_path=f\"hockey_loc{1404}_timesteps.mp4\",\n",
        "    fps=30\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.tensor([\n",
        "        [[2., 2.], [2., 2.]],\n",
        "        [[3., 3.], [3., 3.]],\n",
        "        [[4., 4.], [4., 4.]],\n",
        "        [[5., 5.], [5., 5.]],\n",
        "        [[6., 6.], [6., 6.]],\n",
        "]).unsqueeze(1).repeat((1, 4, 1, 1))\n",
        "\n",
        "scalars = torch.tensor([1/2, 1/3, 1/4, 1/5, 1/6])\n",
        "\n",
        "x = x * scalars.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "x = x.sum(dim=0)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute disk space used for all self-attention maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bytes_per_float = 4\n",
        "\n",
        "# 64 x 64\n",
        "size_64 = 64**4 * num_timesteps * bytes_per_float / 1e9\n",
        "\n",
        "# 32 x 32\n",
        "size_32 = 32**4 * num_timesteps * bytes_per_float / 1e9\n",
        "\n",
        "# 16 x 16\n",
        "size_16 = 16**4 * num_timesteps * bytes_per_float / 1e9\n",
        "\n",
        "# 8 x 8\n",
        "size_8 = 8**4 * num_timesteps * bytes_per_float / 1e9\n",
        "\n",
        "\n",
        "f\"{(size_64 + size_32 + size_16 + size_8):.2f} GB\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot all eight attention heads for a single pixel and time step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = 900\n",
        "\n",
        "# Create a 2 x 4 grid of subplots\n",
        "fig, axs = plt.subplots(2, 4, figsize=(15, 8))\n",
        "fig.suptitle(f\"Attention heads for 64 x 64 map at time step {t}\", fontsize=20)\n",
        "\n",
        "# Loop through heads 0 to 7\n",
        "for i in range(8):\n",
        "    # Compute row and column for subplot\n",
        "    row = i // 4\n",
        "    col = i % 4\n",
        "\n",
        "    # Plotting each head\n",
        "    axs[row, col].imshow(self_attn_dict[t][64][0][i].reshape(64, 64, -1)[:, :, 2700])\n",
        "    axs[row, col].set_title(f'Head {i}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "r52zlVDUJl_9",
        "8s6-2GsMiEwi",
        "YqhfoJBLPDkj",
        "HkDuWW_S6mFK",
        "Ir_E8vlvmC3i",
        "Jq_UYaw9z9W1",
        "AeVPOortS0Xg",
        "drmyt0Zq-Yuf",
        "F_2rE9z3lJxp",
        "gwq2tPWBoUCs",
        "ZuURD2pAgE6S",
        "MhoxFI32WrsZ",
        "KfCj5i1be890",
        "i9M17URAGrRT",
        "ZMg1NFyON8ve",
        "2uO8K_INONGB",
        "n4Mmj0o4OYWu",
        "25FBAbAWOjQT",
        "zdgg-Yirfj1s",
        "aiNXdN1UckMN",
        "XBW0qHSxlF0Z",
        "G4ktTGCsHjZU",
        "JTOmRatCdsap",
        "mc1OqXkMsik4",
        "EVTDEno2spnw"
      ],
      "last_runtime": {
        "build_target": "//experimental/humaninterface/explorations/diffseg/colab_runtime:ml_notebook",
        "kind": "private"
      },
      "name": "diffusion_inference_coco_cityscapes.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_coco.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1689088568669
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_coco.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1688922575491
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_coco.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1688833060440
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687892328479
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687837396445
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687364924074
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687114629456
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1686761132459
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1686163689880
        },
        {
          "file_id": "1z1ERc9A5S1u5ci0dHLSoqDgPeXpUI_lY",
          "timestamp": 1686159970456
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
